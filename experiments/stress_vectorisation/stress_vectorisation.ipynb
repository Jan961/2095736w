{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from hdimvis.metrics.stress.stress import vectorised_stress,unvectorised_stress\n",
    "from hdimvis.data_fetchers.DataFetcher import DataFetcher\n",
    "from hdimvis.metrics.distance_measures.euclidian_and_manhattan import euclidean,manhattan\n",
    "import numpy as np\n",
    "import tracemalloc\n",
    "from time import perf_counter\n",
    "import definitions\n",
    "import os\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Fetching the \"mnist\" dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\.virtualenvs\\2095736w-0SnFieZ0\\lib\\site-packages\\sklearn\\datasets\\_openml.py:932: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Dataset loaded\n",
      "Dataset shape: (70000, 784)\n",
      "####################\n"
     ]
    }
   ],
   "source": [
    "large_dataset = DataFetcher.fetch_data('mnist', size='max')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sizes = np.arange(10,2000, 20)\n",
    "print(sizes)\n",
    "sizes = [3000]\n",
    "num_repeats = 6\n",
    "\n",
    "# 3 for memory use base (index 0), memory use peak (1) and time (index 2),\n",
    "vectorised_data = np.zeros((len(sizes),3,num_repeats))\n",
    "un_vectorised_data = np.zeros((len(sizes),3,num_repeats))\n",
    "\n",
    "collected_data = [vectorised_data, un_vectorised_data]\n",
    "functions = [vectorised_stress, unvectorised_stress]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "for i, size in enumerate(sizes):\n",
    "    for j in range(2):\n",
    "        for k in range(num_repeats):\n",
    "            print(size)\n",
    "            sample_indices = np.random.randint(0, 69999, size)\n",
    "            sample = large_dataset.data[sample_indices]\n",
    "            ld_positions = 20*np.random.rand(sample.shape[0],2)\n",
    "            stress_fn = functions[j]\n",
    "\n",
    "            tracemalloc.start()\n",
    "            stress1 = stress_fn(sample, ld_positions, euclidean)\n",
    "            collected_data[j][i,0,k] = tracemalloc.get_traced_memory()[0]\n",
    "            collected_data[j][i,1,k] = tracemalloc.get_traced_memory()[1]\n",
    "            tracemalloc.stop()\n",
    "\n",
    "            start = perf_counter()\n",
    "            stress2 = stress_fn(sample, ld_positions, euclidean)\n",
    "            collected_data[j][i,2,k] = perf_counter() - start\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "output_dir = os.path.realpath(os.path.join(definitions.PROJECT_ROOT, \"experiments/stress_vectorisation/out/stress_vec_1.npy\"))\n",
    "\n",
    "with open(output_dir, 'wb') as f:\n",
    "    np.save(f, vectorised_data)\n",
    "    np.save(f, un_vectorised_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "size = 3000\n",
    "sample_indices = np.random.randint(0, 69999, size)\n",
    "sample = large_dataset.data[sample_indices]\n",
    "ld_positions = 20*np.random.rand(sample.shape[0],2)\n",
    "stress_ts = vectorised_stress(sample, ld_positions, euclidean)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
