{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from hdimvis.metrics.stress.stress import vectorised_stress,unvectorised_stress\n",
    "from hdimvis.data_fetchers.DataFetcher import DataFetcher\n",
    "from hdimvis.metrics.distance_measures.euclidian_and_manhattan import euclidean,manhattan\n",
    "import numpy as np\n",
    "import tracemalloc\n",
    "from time import perf_counter\n",
    "import definitions\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Fetching the \"mnist\" dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\.virtualenvs\\2095736w-0SnFieZ0\\lib\\site-packages\\sklearn\\datasets\\_openml.py:932: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Dataset loaded\n",
      "Dataset shape: (70000, 784)\n"
     ]
    }
   ],
   "source": [
    "large_dataset = DataFetcher.fetch_data('mnist', size='max')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10 11 12 13 14]\n"
     ]
    }
   ],
   "source": [
    "sizes = np.arange(10,2000, 50)\n",
    "print(sizes)\n",
    "\n",
    "\n",
    "vectorised_data = np.zeros((len(sizes),2)) # two for memory use (index 0) and time (index 1)\n",
    "un_vectorised_data = np.zeros((len(sizes),2))\n",
    "\n",
    "collected_data = [vectorised_data, un_vectorised_data]\n",
    "functions = [vectorised_stress, unvectorised_stress]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4744, 1325040)\n",
      "(3515, 269667)\n",
      "(248, 1585648)\n",
      "(2720, 293928)\n",
      "(248, 1874344)\n",
      "(2728, 319088)\n",
      "(248, 2188144)\n",
      "(2792, 344304)\n",
      "(248, 2527048)\n",
      "(2856, 369520)\n"
     ]
    }
   ],
   "source": [
    "for i, size in enumerate(sizes):\n",
    "    for j in range(2):\n",
    "        sample_indices = np.random.randint(0, 69999, size)\n",
    "        sample = large_dataset.data[sample_indices]\n",
    "        ld_positions = 20*np.random.rand(sample.shape[0],2)\n",
    "        stress_fn = functions[j]\n",
    "\n",
    "        tracemalloc.start()\n",
    "        stress1 = stress_fn(sample, ld_positions, euclidean)\n",
    "        collected_data[j][i,0] = tracemalloc.get_traced_memory()[1]\n",
    "        print(tracemalloc.get_traced_memory())\n",
    "        tracemalloc.stop()\n",
    "\n",
    "\n",
    "\n",
    "        start = perf_counter()\n",
    "        stress2 = stress_fn(sample, ld_positions, euclidean)\n",
    "        collected_data[j][i,1] = perf_counter() - start\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "output_dir = os.path.realpath(os.path.join(definitions.PROJECT_ROOT, \"experiments/stress_vectorisation/out/stress_vec.npy\"))\n",
    "\n",
    "with open(output_dir, 'wb') as f:\n",
    "    np.save(f, vectorised_data)\n",
    "    np.save(f, un_vectorised_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.32504000e+06 1.96900000e-04]\n",
      " [1.58564800e+06 5.86900002e-04]\n",
      " [1.87434400e+06 7.36700000e-04]\n",
      " [2.18814400e+06 6.43200001e-04]\n",
      " [2.52704800e+06 8.49500000e-04]]\n",
      "[[2.69667e+05 3.31880e-03]\n",
      " [2.93928e+05 3.67800e-03]\n",
      " [3.19088e+05 4.16220e-03]\n",
      " [3.44304e+05 5.07510e-03]\n",
      " [3.69520e+05 5.75470e-03]]\n"
     ]
    }
   ],
   "source": [
    "with open(output_dir, 'rb') as f:\n",
    "    a = np.load(f)\n",
    "    b= np.load(f)\n",
    "print(a)\n",
    "print(b)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.32205600e+06 4.06300000e-04]\n",
      " [3.14468560e+07 8.99520000e-03]\n",
      " [1.25600632e+08 3.71306000e-02]]\n"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
