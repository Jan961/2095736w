% REMEMBER: You must not plagiarise anything in your report. Be extremely careful.
\documentclass{l4proj}

    
%==============================================================================
% Put any additional packages here
% You can add any packages you want, as long as it does not alter
% the overall format (e.g. don't change the margins or the reference style).
%
\usepackage{pdfpages} % if you want to include a PDF for an ethics checklist,
%
%

\begin{document}

%==============================================================================
%% METADATA
\title{Level 4 Project Report } % change this to your title
\author{Jan Wieczorek}
\date{September 14, 2018}

\maketitle

%==============================================================================
%% ABSTRACT
\begin{abstract}
    The aim was of the first stage of the project was to compare some aspects of the multidimensional scaling algorithms first presented by \citet{Chalmers96} and \citet{hyrid} and implemented in Python by \citet{2019} with the algorithm developed by \citet{squad}. as well as investigating the impact on the algorithms' performance of various additions and modifications. In the second stage ...
\end{abstract}

%==============================================================================
%% ACKNOWLEDGEMENTS
\chapter*{Acknowledgements}
% Enter any acknowledgements here. This is optional; you may leave this blank if you wish,
% or remove the entire chapter
%
% We give thanks to the Gods of LaTeX, who in their eternal graciousness, 
% have granted that this document may compile without errors or overfull hboxes.
%

%==============================================================================

% EDUCATION REUSE CONSENT FORM
% If you consent to your project being shown to future students for educational purposes
% then insert your name and the date below to  sign the education use form that appears in the front of the document. 
% You must explicitly give consent if you wish to do so.
% If you sign, your project may be included in the Hall of Fame if it scores particularly highly.
%
% Please note that you are under no obligation to sign 
% this declaration, but doing so would help future students.
%
\def\consentname {Jan Wieczorek} % your full name
\def\consentdate {10 March 2023} % the date you agree
%
\educationalconsent


%==============================================================================
\tableofcontents

%==============================================================================
%% Notes on formatting
%==============================================================================
% The first page, abstract and table of contents are numbered using Roman numerals and are not
% included in the page count. 
%
% From now on pages are numbered
% using Arabic numerals. Therefore, immediately after the first call to \chapter we need the call
% \pagenumbering{arabic} and this should be called once only in the document. 
%
%
% The first Chapter should then be on page 1. 

% PAGE LIMITS
% You are allowed 40 pages for a 40 credit project and 30 pages for a 
% 20 credit report. 
% This includes everything numbered in Arabic numerals (excluding front matter) up
% to but *excluding the appendices and bibliography*.
%
% FORMATTING
% You must not alter text size (it is currently 10pt) or alter margins or spacing.
% Do not alter the bibliography style. 
%
%==================================================================================================================================
%
% IMPORTANT
% The chapter headings and structure here are **suggestions**. You don't have to follow this model if
% it doesn't fit your project. Every project should have an introduction and conclusion,
% however.  If in doubt, your supervisor can give you specific guidance; their view takes precedence over
% the structure suggested here.
%
%==================================================================================================================================
\chapter{Introduction}

% reset page numbering. Don't remove this!
\pagenumbering{arabic} 

% You can use \todo{} to mark text that needs to be fixed. Anything inside will appear as highlighted 
% text in the final copy, and you will also get warnings when you compile (so you don't
% forget to take them out!)

\section{Overview and context}

With the continually increasing volume of data available in many academic as well as commercial contexts, there is a great demand for tools to efficiently process and extract useful information from multidimensional data sets. One interesting problem in this area is high-dimensional data visualisation. Since approaches such mosaic/pair plots or using shape, colour etc. to represent dimensions above 2, can only be useful for a small number of dimensions, this almost always involves some form of dimension reduction and generation of a 2D plot. 

As many state-of-the-art solutions to problems involving mapping high-dimensional inputs to lower-dimensional outputs make use of neural networks and deep learning, it is not surprising that a number of attempts have been made to apply these tools, notably Variational Autoencoder architectures, to visualisation \citep{ nn_vis1, nn_vis2, nn_vis3, nn_vis4, nn_vis5, nn_vis6, nn_vis7}. At least in some instances, however, the disadvantages of these approaches, while not unique to neural networks, can, nonetheless, include long processing times \citep{nn_vis5}, the need to tune multiple hyper-parameters that can radically alter the resulting 2D layouts \citep{nn_vis5, nn_vis6} and, potentially, lack of genralisablity as the network architecture might have to be altered and hyper-parameters readjusted for different data types. (examples to add)

The alternative, more popular, approach is to use one of the incredibly large number of available algorithms that do not require training a neural network. These can be, broadly, split into at least two major groups. The first - containing linear algorithms - includes one of the most commonly used dimensionality reduction procedures  - Principal Component Analysis (PCA) \citep{pca}, which, while useful in visualisation, necessarily produces restricted, considerably simplified plots when treated as a standalone visualisation technique. Another linear method - classical Multidimensional Scaling (MDS) \citep{mds} was designed to preserve all pairwise distance relationships between the high-dimensional data points. It often outputs layouts of much better quality, representing both local and global structures fairly accurately. This however comes at the cost of processing time, which has the complexity of O(n\textsuperscript{3}) or O(n\textsuperscript{4}) for most classical MDS algorithms, making this technique impractical to use on large data sets.

Among non-linear methods, one class of iterative algorithms, which includes all three that are the main focus of this project - \citet{Chalmers96}, \citet{hyrid} and \citet{squad} introduces various heuristics and elements of stochasticity to the basic non-metric MDS procedure in an attempt to reduce its prohibitively large time complexity, without sacrificing layout quality. While largely successful in this task, like all visualisation algorithms based on MDS or other linear methods, these techniques tend to perform worse when the high-dimensional structures (manifolds) they aim to represent cannot be regarded as linearly or approximately linearly embedded in the observation space \citep{swissroll} - such as for example a world map rolled into a 3d swiss roll - see section 428742adasfa - to be added).  

The two most popular visualisation algorithms - UMAP \citep{umap} and especially t-SNE \citep{tsne} - as well as accelerated versions of the latter by \citet{barneshutsne} and \citet{fitsne}, are also non-linear. In contrast to the roughly equivalent focus on global and local structures of the MDS-based techniques, the above two algorithms tend to favour the latter at the expense of the former, requiring a suitable initialisation to achieve a reasonable degree of global structure preservation \citep{initialisationcritical} - Figure \ref{fig:init_demo}. At least due to their local focus the two algorithms should also be able to better handle nonlinear data manifolds \citep{swissroll}.


\begin{figure}[htb]
    \centering
    \includegraphics[width=0.3\linewidth]{images/3dcube.png} \\
    \includegraphics[width=0.3\linewidth]{images/PCA.png}
    \includegraphics[width=0.3\linewidth]{images/scikit-learn's MDS.png} 
    \includegraphics[width=0.3\linewidth]{images/Chalmers' 96.png}\\
    \includegraphics[width=0.3\linewidth]{images/umap_cube.png} 
    \includegraphics[width=0.3\linewidth]{images/Stochastic quartet descent.png}
    \includegraphics[width=0.3\linewidth]{images/tsne_cube.png}\\

    \caption{An illustration (with reduced point density) of a set of 5x10\textsuperscript{4} non-overlapping, 3-dimensional points forming 5 faces of a cube and 2D embeddings produced by passing the same 3000 points sampled from the cube to various dimension reducing algorithms. UMAP was initialised with Laplacian Eigenmaps \citep{laplacian_eigen}, t-SNE and Stochastic quartet MDS with PCA. Chalmers' 96 MDS and scikit-learn's classical SMACOF MDS used random initialisations.
    }

    % use the notation fig:name to cross reference a figure
    \label{fig:init_demo} 
\end{figure}

\section{Motivation}

The aim of this project was jnjjfkjakbdsahbsdkajsb

Given the large number of parameters and settings governing the behaviour of each algorithm only the effect of more interesting or significant adjustments was formally tested and many others especially those whose effect was unambiguously detrimental were only investigated by manually running the algorithm on a few datasets. 
%==================================================================================================================================
\chapter{Background}

\section{Stress and other metrics.}

Like most MDS-based dimensionlity reduction methods the three considered algorithms use some variant of the loss function called "stress", first proposed by \citet{og_stress} and defined as: 

\begin{equation}
\label{tab:stress}
    Stress = \sqrt{\frac{\sum_{i<j}(\delta_{ij} - d_{ij})^2 }{\sum_{i<j}d_{ij}^2}},
\end{equation}    
where $\delta_{ij}$ is the high-dimensional distance between data points i and j and $d_{ij}$ is the distance between their low-dimensional representations. It is worth noting that, while enormously informative, such defined "stress" is not a perfect measure for assessing the quality of the 2D layouts, not least because this quality depends in part on the subjective judgement of the viewer and the purpose of visualisation (some systematic limitations are briefly illustrated in section 823492sss). This is one reason why it is common in  high-dimensional data visualisation literature to include qualitative evaluation of algorithms, often on simplified, synthetic data-sets, as part of the overall assessment of the algorithms' performance. For quantitative assessment a wide variety of, not infrequently custom designed, metrics is used. Many of those metrics highlight different aspects of the produced layouts. 

In this project three synthetic data sets were used: a 3d cube, a 3d globe and a 2d world map rolled into a 3d Swiss roll \footnote{"Swiss roll world map" idea from: \url{https://github.com/NikolayOskolkov/tSNE_vs_UMAP_GlobalStructure}}. Apart from Stress the layouts were quantitatively evaluated by comparing their accuracy with k-NND algorithm - as in (reference to dig out) and by (nabsjnfanakan to add)

\section{Algorithms}


\subsection{\cite{Chalmers96}}

This algorithm relies on a spring force model to translate the high-dimensional distance structure of a data-set into two dimensions. To reduce the number of comparisons performed in the classical MDS, each data-point (or node) $i$ is assigned two non-intersecting sets of other nodes: a neighbour set $V_i$ and random sample set $S_i$. At each iteration a new sample is drawn for each node $i$ and the neighbour set $V_i$ is updated in case any closer neighbours are found. A force equal to the sum of all forces between the node i and each node j in either $S_i$ or $V_i$ is computed. Force between nodes i and j is defined as:
\begin{equation}
        F_{ij} = ()
\end{equation}  
where $(k_{s}$ is a spring constant, is $(k_{d}$ a damping constant l is bla blah and $v_{ij}$ is the sum of the magnitudes of components of velocities of i and j in the direction of l. The implementation of the algorithm used in this project \citep{2019} did not include a spring force between each node the 2D plane onto which the layout is projected, unlike in the original paper \citep{Chalmers96}. As noticed by \citet{2019} in this implementation the 2D layouts stop evolving after around 100 (much smaller than N) iterations for most data-sets and thus, since the sizes of $S_i$ and $V_i$ are kept constant, the algorithm effectively operates in linear time rather than $O(N^2)$ as described by \citet{Chalmers96}. 

The role of $k_{d}$ is to prevent the system from becoming unstable (see section 412311) by reducing the overall force on a node. Nonetheless it was found that best results where produced when $k_{d}$ was set to zero and $k_{s}$ was directly decreased instead, as in the original implementation by \citet{2019}. Similarly, initial experiments revealed that  thus this setting was also left unchanged. Finally,  again as in the original implementation, for simplicity, the integration stop is set to one, so that force, velocity and position update of a node become equal. 

\subsection{\cite{hyrid}}

Here the runtime of \citet{Chalmers96} is further reduced by only applying the full layout algorithm on a $\sqrt{N}$ subset of the dataset, interpolatin the remaining datapoints into the layout and finally gghhg. During interpolation stage for each datapoint not yet placed on the 2D layout ghhgg. Further details are discussed in (ghghjhu). As before since the original specification assumes \citet{Chalmers96} to run in $O(n^2)$ time this modification reduced the time complexity to $O(n\log n)$ but in \citet{2019} implementation it reduces it to a shorter linear time.

\subsection{\cite{squad} - Stochastic Quartet Descent MDS (SQuaD)}

This is, in many ways, a similar approach to reducing the time complexity of classical MDS to that employed by \citet{Chalmers96}. Instead of using a heuristic of spring forces based on $V_i$ and $S_i$, however, at each iteration this algorithm randomly partitions the dataset into groups of four (quartets) and updates the low dimensional position of each datapoint in a quartet simultaneously, directly based on the gradients of the stress function calculated for the quartet, which is defined as:
\begin{equation}
\label{tab:quartet_stress}
    Stress_{quartet} = \sum_{a\, =\, 1}^{3} \sum_{b\, =\, a\, +\, 1}^{4} (\delta_{ij}^{\,r} - d_{ij}^{\,r})^2,
\end{equation} 
Here $\delta^{r}$ and $d^{r}$ are matrices of relative, that is normalised by the sum of all distances in the quartet, respectively high- and low-dimensional distances between datapoints i and j. The total stress of the layout is then defied as the average of all quartet stresses. Since the number of iterations required for convergence is around 500 and independent of N (?) this is, again, a linear time algorithm. The code provided by the authors \footnote{ \url{https://github.com/PierreLambert3/SQuaD-MDS}} served a the starting point for the implementation used in this project.


%==================================================================================================================================
\chapter{Design}



\begin{figure}[htb]
    \centering
    \includegraphics[width=1\linewidth]{images/class diag.png} \\
 
    \caption{A simplified class diagram for the project showing the key classes and the separation of concerns. Red classes are responsible for data pre-processing and loading, blue contain the main logic of all algorithms and green create layouts as well as collect various measurements during the process.
    }

    % use the notation fig:name to cross reference a figure
    \label{fig:uml} 
\end{figure}





What is the problem that you want to solve, and how did you arrive at it?
\section{Guidance}
Make it clear how you derived the constrained form of your problem via a clear and logical process. 

The analysis chapter explains the process by which you arrive at a concrete design. In software 
engineering projects, this will include a statement of the requirement capture process and the
derived requirements.

In research projects, it will involve developing a design drawing on
the work established in the background, and stating how the space of possible projects was
sensibly narrowed down to what you have done.

%==================================================================================================================================
\chapter{Experiments}

\section{Measurement Challenges and Scope of the Project}

A comparison between any two high-dimensional data visualisation algorithms could focus on many aspects of their performance. One such important aspect is memory use. However, despite the existence of various measurement tools for Python, obtaining accurate figures is not easy \footnote{ \url{https://pythonspeed.com/articles/measuring-memory-python/}}. Some tools only measure RSS memory use, others limit themselves to memory blocks allocated by Python interpreter itself, not including those used by libraries written in different languages, while yet others are only available for specific platforms, such as Linux. This is why early on in the project it was decided that memory use measurement will not be a major focus of this work.

More broadly, given that the algorithm implementations by \citet{2019} and the \citet{squad} were written using different styles, the sheer number of possible tiny alteration which might or might not improve efficiency of either of them and the fact that the algorithms were broadly equally efficient in their existing implementations; it seemed that the project could take at least three main approaches to comparing the algorithms.

First, in order to make the comparison between SQuaD and the spring forces algorithms implemented by \citet{2019} fairer, would involve completely rewriting the for code either of them - e.g. the latter to use a popular Python acceleration library Numba \footnote{ \url{https://numba.pydata.org/}}.

Second would explicitly focus on making both algorithms' implementations as efficient as possible, perhaps with the restriction of only using Python API and Python libraries, and then measuring their relative performance.

Third would generally only alter the high-level logic of the algorithms, focus on the quality of the produced embedding/layouts rather than efficiency and use the existing implementations as baselines.

It was decided the the last option would be the most interesting and useful one to take.

\section{Stress Vectorisation with Numpy}

Despite the aforementioned lack of specific focus on efficiency, one aspect of the computations performed where a decrease in execution time presented significant benefits was calculation of Stress (\ref{tab:stress}) of a layout. Stress calculation is an expensive $O(n^2)$ operation which if performed periodically during layout creation tended to take up the majority of runtime. Thus the original implementation using the itertools library \footnote{ \url{https://docs.python.org/3/library/itertools.html}} and a Python loop, was rewritten with Numpy \footnote{ \url{https://numpy.org/}} as in Listing \ref{lst:vectorised_stress}. This provided an interesting opportunity to compare resource use by the two methods. The experiment was performed on a machine with 32.0 GB (31.7 GB usable) physical RAM, using the Python tool tracemalloc which, as related to the discussion in the previous section, does track memory allocated to Numpy {\footnote{\url{https://docs.python.org/3/library/tracemalloc.html}}. As expected the vectorised method used much more - up to 3 orders of magnitude - memory but was significantly faster. However at the level of memory usage of around 26 GB the Numpy method performance begins to degrade rapidly - which must be caused primarily by the increasing use of swap space - until finally at levels above around 40GB an insufficient memory exception is thrown. In such cases, in this project, the original calculation method is used as a fallback.

\begin{lstlisting}[language=python, float, caption={Python code to calculate vectorised Stress. A "distance\_function" retruns a norm of a numpy array (1st parameter) calculated along the specified axis (2nd parameter).}, label=lst:vectorised_stress]
def vectorised_stress(data: np.ndarray, ld_positions: np.ndarray, distance_function: Callable):
    
    hd_dist = distance_function(data[:,:,None] - data[:,:,None].T, 1)
    ld_dist = distance_function(ld_positions[:,:,None] - ld_positions[:,:,None].T, 1)
    numerator = np.sum((hd_dist - ld_dist)**2)
    denominator = np.sum(ld_dist**2)
    if denominator == 0:
        return np.inf
    else:
        return np.sqrt(numerator/denominator)
    
\end{lstlisting}

\section{Stochastic "N-tet" Descent MDS}

In this experiment the effect of changing the original size  (four) of the base unit used for layout creation by SQuaD was explored. Thus the SQuaD algorithm adapted for an arbitrary unit size greater than 3 was named Stochastic "N-tet" Descent (SNeD) - where "n-tet" stands for: trio, quartet, quintet, sextet and so on.  

Taking the partial derivative of one part of the loss function sum (\ref{tab:quartet_stress}) adapted for a an arbitrary n-tet size $k$, with respect to one of the two dimensions on the 2D layout, e.g. $x$, of the $q^{th}$ point in the n-tet, results in: 

\begin{equation}
\label{tab:adapted quartet stress}
    \frac{\partial (\delta_{ij}^{\,r} - d_{ij}^{\,r})^2}{\partial x_q} = \frac{2}{\sum_{i<j}d_{ij}}(\delta_{ij}^{\,r} - d_{ij}^{\,r})\left(\frac{\partial d_{ij}}{\partial x_q} +  \frac{\partial \sum_{i<j}}{\partial x_q}d_{ij}^{\,r}\right), 
\end{equation}

\begin{center} which can be further reduced and re-arranged to: \end{center}

\begin{equation}
\label{tab:adapted quartet stress expanded}
     \frac{2}{\sum_{i<j}d_{ij}}(\delta_{ij}^{\,r} - d_{ij}^{\,r}) \left(\frac{I_{qi}(x_q - x_j) + I_{qj}(x_q - x_i)}{d_{ij}}  -  d_{ij}^{\,r} \sum_{b\in\{1,...,k\}\setminus q} \frac{x_q - x_b}{d_{qb}} \right), 
\end{equation} 

as in \citet{squad}. $\delta$ and $d$ represent the matrices of respectively, high- and low-dimensional distances between n-tet points and, as before,  $\delta^{r}$ and $d^{r}$ are their relativized equivalents. The identity matrix $I$, of dimensions $k$x$k$, is a concise way of representing how the formula changes depending on whether $q$ is equal to either $i$ or $j$. While adapting the gradient formula for an arbitrary $k$ was trivial this modification required much more substantial changes to the code base. The




%==================================================================================================================================
\chapter{Implementation}
What did you do to implement this idea, and what technical achievements did you make?
\section{Guidance}
You can't talk about everything. Cover the high level first, then cover important, relevant or impressive details.

\section{General guidance for technical writing}

These points apply to the whole dissertation, not just this chapter.

\subsection{Figures}
\emph{Always} refer to figures included, like Figure \ref{fig:relu}, in the body of the text. Include full, explanatory captions and make sure the figures look good on the page.
You may include multiple figures in one float, as in Figure \ref{fig:synthetic}, using \texttt{subcaption}, which is enabled in the template.


% Figures are important. Use them well.
\begin{figure}[htb]
    \centering
    \includegraphics[width=0.5\linewidth]{images/relu.pdf}    

    \caption{In figure captions, explain what the reader is looking at: ``A schematic of the rectifying linear unit, where $a$ is the output amplitude,
    $d$ is a configurable dead-zone, and $Z_j$ is the input signal'', as well as why the reader is looking at this: 
    ``It is notable that there is no activation \emph{at all} below 0, which explains our initial results.'' 
    \textbf{Use vector image formats (.pdf) where possible}. Size figures appropriately, and do not make them over-large or too small to read.
    }

    % use the notation fig:name to cross reference a figure
    \label{fig:relu} 
\end{figure}


\begin{figure}[htb] 
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{images/synthetic.png}
        \caption{Synthetic image, black on white.}
        \label{fig:syn1}
    \end{subfigure}
    ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
      %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{images/synthetic_2.png}
        \caption{Synthetic image, white on black.}
        \label{fig:syn2}
    \end{subfigure}
    ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
    %(or a blank line to force the subfigure onto a new line)    
    \caption{Synthetic test images for edge detection algorithms. \subref{fig:syn1} shows various gray levels that require an adaptive algorithm. \subref{fig:syn2}
    shows more challenging edge detection tests that have crossing lines. Fusing these into full segments typically requires algorithms like the Hough transform.
    This is an example of using subfigures, with \texttt{subref}s in the caption.
    }\label{fig:synthetic}
\end{figure}

\clearpage

\subsection{Equations}

Equations should be typeset correctly and precisely. Make sure you get parenthesis sizing correct, and punctuate equations correctly 
(the comma is important and goes \textit{inside} the equation block). Explain any symbols used clearly if not defined earlier. 

For example, we might define:
\begin{equation}
    \hat{f}(\xi) = \frac{1}{2}\left[ \int_{-\infty}^{\infty} f(x) e^{2\pi i x \xi} \right],
\end{equation}    
where $\hat{f}(\xi)$ is the Fourier transform of the time domain signal $f(x)$.

\subsection{Algorithms}
Algorithms can be set using \texttt{algorithm2e}, as in Algorithm \ref{alg:metropolis}.

% NOTE: line ends are denoted by \; in algorithm2e
\begin{algorithm}
    \DontPrintSemicolon
    \KwData{$f_X(x)$, a probability density function returning the density at $x$.\; $\sigma$ a standard deviation specifying the spread of the proposal distribution.\;
    $x_0$, an initial starting condition.}
    \KwResult{$s=[x_1, x_2, \dots, x_n]$, $n$ samples approximately drawn from a distribution with PDF $f_X(x)$.}
    \Begin{
        $s \longleftarrow []$\;
        $p \longleftarrow f_X(x)$\;
        $i \longleftarrow 0$\;
        \While{$i < n$}
        {
            $x^\prime \longleftarrow \mathcal{N}(x, \sigma^2)$\;
            $p^\prime \longleftarrow f_X(x^\prime)$\;
            $a \longleftarrow \frac{p^\prime}{p}$\;
            $r \longleftarrow U(0,1)$\;
            \If{$r<a$}
            {
                $x \longleftarrow x^\prime$\;
                $p \longleftarrow f_X(x)$\;
                $i \longleftarrow i+1$\;
                append $x$ to $s$\;
            }
        }
    }
    
\caption{The Metropolis-Hastings MCMC algorithm for drawing samples from arbitrary probability distributions, 
specialised for normal proposal distributions $q(x^\prime|x) = \mathcal{N}(x, \sigma^2)$. The symmetry of the normal distribution means the acceptance rule takes the simplified form.}\label{alg:metropolis}
\end{algorithm}

\subsection{Tables}

If you need to include tables, like Table \ref{tab:operators}, use a tool like https://www.tablesgenerator.com/ to generate the table as it is
extremely tedious otherwise. 

\begin{table}[]
    \caption{The standard table of operators in Python, along with their functional equivalents from the \texttt{operator} package. Note that table
    captions go above the table, not below. Do not add additional rules/lines to tables. }\label{tab:operators}
    %\tt 
    \rowcolors{2}{}{gray!3}
    \begin{tabular}{@{}lll@{}}
    %\toprule
    \textbf{Operation}    & \textbf{Syntax}                & \textbf{Function}                            \\ %\midrule % optional rule for header
    Addition              & \texttt{a + b}                          & \texttt{add(a, b)}                                    \\
    Concatenation         & \texttt{seq1 + seq2}                    & \texttt{concat(seq1, seq2)}                           \\
    Containment Test      & \texttt{obj in seq}                     & \texttt{contains(seq, obj)}                           \\
    Division              & \texttt{a / b}                          & \texttt{div(a, b) }  \\
    Division              & \texttt{a / b}                          & \texttt{truediv(a, b) } \\
    Division              & \texttt{a // b}                         & \texttt{floordiv(a, b)}                               \\
    Bitwise And           & \texttt{a \& b}                         & \texttt{and\_(a, b)}                                  \\
    Bitwise Exclusive Or  & \texttt{a \textasciicircum b}           & \texttt{xor(a, b)}                                    \\
    Bitwise Inversion     & \texttt{$\sim$a}                        & \texttt{invert(a)}                                    \\
    Bitwise Or            & \texttt{a | b}                          & \texttt{or\_(a, b)}                                   \\
    Exponentiation        & \texttt{a ** b}                         & \texttt{pow(a, b)}                                    \\
    Identity              & \texttt{a is b}                         & \texttt{is\_(a, b)}                                   \\
    Identity              & \texttt{a is not b}                     & \texttt{is\_not(a, b)}                                \\
    Indexed Assignment    & \texttt{obj{[}k{]} = v}                 & \texttt{setitem(obj, k, v)}                           \\
    Indexed Deletion      & \texttt{del obj{[}k{]}}                 & \texttt{delitem(obj, k)}                              \\
    Indexing              & \texttt{obj{[}k{]}}                     & \texttt{getitem(obj, k)}                              \\
    Left Shift            & \texttt{a \textless{}\textless b}       & \texttt{lshift(a, b)}                                 \\
    Modulo                & \texttt{a \% b}                         & \texttt{mod(a, b)}                                    \\
    Multiplication        & \texttt{a * b}                          & \texttt{mul(a, b)}                                    \\
    Negation (Arithmetic) & \texttt{- a}                            & \texttt{neg(a)}                                       \\
    Negation (Logical)    & \texttt{not a}                          & \texttt{not\_(a)}                                     \\
    Positive              & \texttt{+ a}                            & \texttt{pos(a)}                                       \\
    Right Shift           & \texttt{a \textgreater{}\textgreater b} & \texttt{rshift(a, b)}                                 \\
    Sequence Repetition   & \texttt{seq * i}                        & \texttt{repeat(seq, i)}                               \\
    Slice Assignment      & \texttt{seq{[}i:j{]} = values}          & \texttt{setitem(seq, slice(i, j), values)}            \\
    Slice Deletion        & \texttt{del seq{[}i:j{]}}               & \texttt{delitem(seq, slice(i, j))}                    \\
    Slicing               & \texttt{seq{[}i:j{]}}                   & \texttt{getitem(seq, slice(i, j))}                    \\
    String Formatting     & \texttt{s \% obj}                       & \texttt{mod(s, obj)}                                  \\
    Subtraction           & \texttt{a - b}                          & \texttt{sub(a, b)}                                    \\
    Truth Test            & \texttt{obj}                            & \texttt{truth(obj)}                                   \\
    Ordering              & \texttt{a \textless b}                  & \texttt{lt(a, b)}                                     \\
    Ordering              & \texttt{a \textless{}= b}               & \texttt{le(a, b)}                                     \\
    % \bottomrule
    \end{tabular}
    \end{table}
\subsection{Code}

Avoid putting large blocks of code in the report (more than a page in one block, for example). Use syntax highlighting if possible, as in Listing \ref{lst:callahan}.

\begin{lstlisting}[language=python, float, caption={The algorithm for packing the $3\times 3$ outer-totalistic binary CA successor rule into a 
    $16\times 16\times 16\times 16$ 4 bit lookup table, running an equivalent, notionally 16-state $2\times 2$ CA.}, label=lst:callahan]
    def create_callahan_table(rule="b3s23"):
        """Generate the lookup table for the cells."""        
        s_table = np.zeros((16, 16, 16, 16), dtype=np.uint8)
        birth, survive = parse_rule(rule)

        # generate all 16 bit strings
        for iv in range(65536):
            bv = [(iv >> z) & 1 for z in range(16)]
            a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p = bv

            # compute next state of the inner 2x2
            nw = apply_rule(f, a, b, c, e, g, i, j, k)
            ne = apply_rule(g, b, c, d, f, h, j, k, l)
            sw = apply_rule(j, e, f, g, i, k, m, n, o)
            se = apply_rule(k, f, g, h, j, l, n, o, p)

            # compute the index of this 4x4
            nw_code = a | (b << 1) | (e << 2) | (f << 3)
            ne_code = c | (d << 1) | (g << 2) | (h << 3)
            sw_code = i | (j << 1) | (m << 2) | (n << 3)
            se_code = k | (l << 1) | (o << 2) | (p << 3)

            # compute the state for the 2x2
            next_code = nw | (ne << 1) | (sw << 2) | (se << 3)

            # get the 4x4 index, and write into the table
            s_table[nw_code, ne_code, sw_code, se_code] = next_code

        return s_table

\end{lstlisting}

%==================================================================================================================================
\chapter{Evaluation} 
How good is your solution? How well did you solve the general problem, and what evidence do you have to support that?

\section{Guidance}
\begin{itemize}
    \item
        Ask specific questions that address the general problem.
    \item
        Answer them with precise evidence (graphs, numbers, statistical
        analysis, qualitative analysis).
    \item
        Be fair and be scientific.
    \item
        The key thing is to show that you know how to evaluate your work, not
        that your work is the most amazing product ever.
\end{itemize}

\section{Evidence}
Make sure you present your evidence well. Use appropriate visualisations, 
reporting techniques and statistical analysis, as appropriate. The point is not
to dump all the data you have but to present an argument well supported by evidence gathered.

If you use numerical evidence, specify reasonable numbers of significant digits; don't state ``18.41141\% of users were successful'' if you only had 20 users. If you average \textit{anything}, present both a measure of central tendency (e.g. mean, median) \textit{and} a measure of spread (e.g. standard deviation, min/max, interquartile range).

You can use \texttt{siunitx} to define units, space numbers neatly, and set the precision for the whole LaTeX document. 

% setup siunitx to have two decimal places
\sisetup{
	round-mode = places,
	round-precision = 2
}

For example, these numbers will appear with two decimal places: \num{3.141592}, \num{2.71828}, and this one will appear with reasonable spacing \num{1000000}.



If you use statistical procedures, make sure you understand the process you are using,
and that you check the required assumptions hold in your case. 

If you visualise, follow the basic rules, as illustrated in Figure \ref{fig:boxplot}:
\begin{itemize}
\item Label everything correctly (axis, title, units).
\item Caption thoroughly.
\item Reference in text.
\item \textbf{Include appropriate display of uncertainty (e.g. error bars, Box plot)}
\item Minimize clutter.
\end{itemize}

See the file \texttt{guide\_to\_visualising.pdf} for further information and guidance.

\begin{figure}[htb]
    \centering
    \includegraphics[width=1.0\linewidth]{images/boxplot_finger_distance.pdf}    

    \caption{Average number of fingers detected by the touch sensor at different heights above the surface, averaged over all gestures. Dashed lines indicate
    the true number of fingers present. The Box plots include bootstrapped uncertainty notches for the median. It is clear that the device is biased toward 
    undercounting fingers, particularly at higher $z$ distances.
    }

    % use the notation fig:name to cross reference a figure
    \label{fig:boxplot} 
\end{figure}


%==================================================================================================================================
\chapter{Conclusion}    
Summarise the whole project for a lazy reader who didn't read the rest (e.g. a prize-awarding committee). This chapter should be short in most dissertations; maybe one to three pages.
\section{Guidance}
\begin{itemize}
    \item
        Summarise briefly and fairly.
    \item
        You should be addressing the general problem you introduced in the
        Introduction.        
    \item
        Include summary of concrete results (``the new compiler ran 2x
        faster'')
    \item
        Indicate what future work could be done, but remember: \textbf{you
        won't get credit for things you haven't done}.
\end{itemize}

\section{Summary}
Summarise what you did; answer the general questions you asked in the introduction. What did you achieve? Briefly describe what was built and summarise the evaluation results.

\section{Reflection}
Discuss what went well and what didn't and how you would do things differently if you did this project again.

\section{Future work}
Discuss what you would do if you could take this further -- where would the interesting directions to go next be? (e.g. you got another year to work on it, or you started a company to work on this, or you pursued a PhD on this topic)

%==================================================================================================================================
%
% 
%==================================================================================================================================
%  APPENDICES  

\begin{appendices}

\chapter{Appendices}

Use separate appendix chapters for groups of ancillary material that support your dissertation. 
Typical inclusions in the appendices are:

\begin{itemize}
\item
  Copies of ethics approvals (you must include these if you needed to get them)
\item
  Copies of questionnaires etc. used to gather data from subjects. Don't include
  voluminous data logs; instead submit these electronically alongside your source code.
\item
  Extensive tables or figures that are too bulky to fit in the main body of
  the report, particularly ones that are repetitive and summarised in the body.
\item Outline of the source code (e.g. directory structure), 
    or other architecture documentation like class diagrams.
\item User manuals, and any guides to starting/running the software. 
Your equivalent of \texttt{readme.md} should be included.

\end{itemize}

\textbf{Don't include your source code in the appendices}. It will be
submitted separately.



\end{appendices}

%==================================================================================================================================
%   BIBLIOGRAPHY   

% The bibliography style is agsm (Harvard)
% The bibliography always appears last, after the appendices.

\bibliographystyle{agsm}

% Force the bibliography not to be numbered
\renewcommand{\thechapter}{0} 
\bibliography{l4proj}

\end{document}
